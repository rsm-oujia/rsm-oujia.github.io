---
title: "Poisson Regression Examples"
author: "Ouwen Jia"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
editor: visual
---

## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available.

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.

### Data

```{r load-packages, include=FALSE}
library(ggplot2)
library(readr)
library(dplyr)
```

```{r load-data, include=FALSE}
data <- read_csv("blueprinty.csv")

data <- data %>%
  mutate(
    iscustomer = factor(iscustomer, levels = c(0, 1), labels = c("Non-Customer", "Customer"))
  )
```

#### Distribution of Patents by Customer Status

```{r hist-patents}
ggplot(data, aes(x = patents, fill = iscustomer)) +
  geom_histogram(position = "identity", alpha = 0.5, bins = 30) +
  labs(
    title = "Number of Patents by Customer Status",
    x = "Patents (Last 5 Years)",
    fill = "Customer Status"
  ) +
  theme_minimal()
```

```{r patent-summary}
data %>%
  group_by(iscustomer) %>%
  summarise(
    count = n(),
    mean_patents = mean(patents),
    sd_patents = sd(patents)
  )
```

We observe that Blueprinty customers, on average, have more patents over the past five years. However, this comparison does not yet account for potential confounding variables.

#### Regional Differences by Customer Status

```{r region-bar}
ggplot(data, aes(x = region, fill = iscustomer)) +
  geom_bar(position = "fill") +
  labs(
    title = "Regional Distribution by Customer Status",
    x = "Region",
    y = "Proportion of Firms",
    fill = "Customer Status"
  ) +
  theme_minimal()
```

There are noticeable regional differences in customer adoption. Some regions appear to have a higher share of Blueprinty customers.

#### Firm Age Distribution by Customer Status

```{r age-hist}
ggplot(data, aes(x = age, fill = iscustomer)) +
  geom_histogram(position = "identity", alpha = 0.5, bins = 30) +
  labs(
    title = "Firm Age Distribution by Customer Status",
    x = "Age (Years Since Incorporation)",
    fill = "Customer Status"
  ) +
  theme_minimal()
```

```{r age-summary}
data %>%
  group_by(iscustomer) %>%
  summarise(
    mean_age = mean(age),
    sd_age = sd(age)
  )
```

On average, customers are older firms. Age may therefore be a confounding factor when evaluating patent success.

#### Summary

While customers have more patents on average, they also differ in age and region. These structural differences suggest we should adjust for confounding variables in any attempt to infer a causal impact of Blueprinty's software.

### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

The probability mass function for a Poisson distribution is:

$f(Y | \lambda) = \frac{e^{-\lambda} \lambda^Y}{Y!}$

We treat the observed number of patents as realizations from this distribution.

#### Log-Likelihood Function in R

```{r poisson-loglikelihood}
# Log-likelihood function for Poisson model
poisson_loglikelihood <- function(lambda, Y) {
  if (lambda <= 0) return(-Inf)  # Poisson lambda must be positive
  sum(dpois(Y, lambda, log = TRUE))
}
```

#### Plotting the Log-Likelihood Curve

We use the observed number of patents as input to our log-likelihood function.

```{r plot-loglikelihood}
Y_obs <- data$patents
lambda_vals <- seq(0.1, 10, by = 0.1)
loglik_vals <- sapply(lambda_vals, function(l) poisson_loglikelihood(l, Y_obs))

plot(lambda_vals, loglik_vals, type = "l", lwd = 2,
     main = "Log-Likelihood of Poisson Model",
     xlab = expression(lambda), ylab = "Log-Likelihood")
```

#### Maximum Likelihood Estimation (MLE)

We now use `optim()` to find the value of lambda that maximizes the log-likelihood.

```{r poisson-mle}
mle_result <- optim(par = 1, fn = function(l) -poisson_loglikelihood(l, Y_obs),
                    method = "Brent", lower = 0.01, upper = 20)

lambda_mle <- mle_result$par
lambda_mle
```

The MLE for $\lambda$ is the sample mean of $Y$, which is consistent with theory: for Poisson-distributed data, $\hat{\lambda}_{MLE} = \bar{Y}$.

```{r sample-mean}
mean(Y_obs)
```

### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

#### Log-Likelihood Function with Covariates

```{r poisson-regression-loglik}
poisson_regression_loglik <- function(beta, Y, X) {
  lambda <- exp(X %*% beta)
  sum(dpois(Y, lambda, log = TRUE))
}
```

#### Creating Covariate Matrix X

```{r create-X}
# Age and age squared
data <- data %>%
  mutate(age_sq = age^2)

# One-hot encode region (drop 1 category)
region_dummies <- model.matrix(~ region, data = data)[, -1]

# Final covariate matrix: intercept, age, age_sq, region dummies, customer
X <- cbind(
  intercept = 1,
  age = data$age,
  age_sq = data$age_sq,
  region_dummies,
  iscustomer = as.numeric(data$iscustomer == "Customer")
)

Y <- data$patents
```

#### Find MLE and Standard Errors via `optim()`

```{r poisson-regression-optim}
loglik_wrapper <- function(beta) -poisson_regression_loglik(beta, Y, X)

# Initial guess
beta_init <- rep(0, ncol(X))

# Optimize
opt_result <- optim(par = beta_init, fn = loglik_wrapper, hessian = TRUE, method = "BFGS")

# Coefficients
beta_hat <- opt_result$par

# Standard errors from Hessian
hess_inv <- solve(opt_result$hessian)
se_hat <- sqrt(diag(hess_inv))

# Summary table
coef_table <- data.frame(
  Term = colnames(X),
  Estimate = beta_hat,
  StdError = se_hat
)

coef_table
```

#### Check Using `glm()` in R

```{r poisson-glm}
glm_model <- glm(patents ~ age + I(age^2) + region + iscustomer,
                 family = poisson(link = "log"),
                 data = data)

summary(glm_model)
```

#### Interpretation

This Poisson regression estimates the expected number of patents awarded to firms as a function of age, age squared, region, and Blueprinty software usage. Key insights include:

-   **age**: The coefficient is positive and highly significant. This suggests that older firms tend to have more patents, all else equal. Specifically, a one-year increase in firm age is associated with an expected increase in patents by a factor of `exp(0.1486) ≈ 1.160` (or about a 16% increase).

-   **I(age²)**: The negative and significant coefficient on age squared indicates a concave (inverted U-shaped) relationship. This means that the rate of increase in patents slows down for very old firms.

-   **region**: None of the regional dummy variables (Northeast, Northwest, South, Southwest) are statistically significant. This implies that — after controlling for age and Blueprinty usage — there are no strong regional differences in patent counts.

-   **iscustomerCustomer**: The coefficient is **positive and highly significant** (`p < 0.001`). Using Blueprinty's software is associated with an increase in the expected number of patents. The estimated coefficient `0.208` corresponds to a **23.1% increase** in expected patent count (`exp(0.208) ≈ 1.231`).

#### Predictive Comparison: Blueprinty vs Non-Customer

```{r marginal-effect}
# Create X_0 and X_1 matrices
X_0 <- X
X_0[, "iscustomer"] <- 0

X_1 <- X
X_1[, "iscustomer"] <- 1

# Predicted lambda for each case
lambda_0 <- exp(X_0 %*% beta_hat)
lambda_1 <- exp(X_1 %*% beta_hat)

# Predicted difference
diff <- lambda_1 - lambda_0
mean(diff)
```

This gives the average expected increase in the number of patents due to using Blueprinty's software, across all firms in the dataset.

## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:

::: {.callout-note collapse="true"}
### Variable Definitions

```         
- `id` = unique ID number for each unit
- `last_scraped` = date when information scraped
- `host_since` = date when host first listed the unit on Airbnb
- `days` = `last_scraped` - `host_since` = number of days the unit has been listed
- `room_type` = Entire home/apt., Private room, or Shared room
- `bathrooms` = number of bathrooms
- `bedrooms` = number of bedrooms
- `price` = price per night (dollars)
- `number_of_reviews` = number of reviews for the unit on Airbnb
- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
- `review_scores_location` = a "quality of location" score from reviews (1-10)
- `review_scores_value` = a "quality of value" score from reviews (1-10)
- `instant_bookable` = "t" if instantly bookable, "f" if not
```
:::

### Exploratory Data Analysis

```{r load-data-airbnb}
airbnb <- read_csv("airbnb.csv")

glimpse(airbnb)
summary(airbnb)
```

### Data Cleaning

```{r clean-data}
# Drop rows with missing values in relevant columns
airbnb_clean <- airbnb %>%
  filter(
    !is.na(number_of_reviews),
    !is.na(review_scores_cleanliness),
    !is.na(review_scores_location),
    !is.na(review_scores_value),
    !is.na(bathrooms),
    !is.na(bedrooms),
    !is.na(price)
  ) %>%
  mutate(
    instant_bookable = ifelse(instant_bookable == "t", 1, 0),
    room_type = as.factor(room_type)
  )

summary(airbnb_clean)
```

### Distribution of Reviews

```{r hist-reviews}
ggplot(airbnb_clean, aes(x = number_of_reviews)) +
  geom_histogram(bins = 50, fill = "steelblue", alpha = 0.7) +
  labs(title = "Distribution of Number of Reviews", x = "Number of Reviews", y = "Count") +
  theme_minimal()
```

### Poisson Regression Model

```{r poisson-model}
model <- glm(number_of_reviews ~ days + price + bedrooms + bathrooms +
               review_scores_cleanliness + review_scores_location +
               review_scores_value + instant_bookable + room_type,
             family = poisson(link = "log"),
             data = airbnb_clean)

summary(model)
```

### Interpretation

The Poisson regression model estimates how various listing characteristics influence the expected number of reviews (used as a proxy for bookings). Below are key takeaways:

-   **days**: Longer-listed properties tend to receive more reviews. Each additional day on the platform increases expected reviews by a factor of `exp(4.962e-05) ≈ 1.00005`.

-   **price**: Listings with higher nightly prices receive fewer reviews. The negative and significant coefficient suggests price sensitivity.

-   **bedrooms**: Each additional bedroom increases expected reviews by approximately 7.8% (`exp(0.0756) ≈ 1.0785`).

-   **bathrooms**: Surprisingly, more bathrooms are associated with fewer reviews, which may reflect multicollinearity or data-specific effects.

-   **review_scores_cleanliness**: A 1-point increase in cleanliness rating is associated with a 12% increase in expected reviews (`exp(0.1138) ≈ 1.12`).

-   **review_scores_location** and **review_scores_value**: These have negative coefficients, which may suggest correlation with other factors or overcontrol in the model.

-   **room_type**:

    -   *Private room*: Increases reviews slightly compared to entire homes (`exp(0.0213) ≈ 1.0215`).
    -   *Shared room*: Associated with significantly fewer reviews (`exp(-0.2172) ≈ 0.805`).

-   **instant_bookable**: Dropped from the model due to collinearity, indicating redundancy or lack of variation.

### Conclusion

The analysis shows that Airbnb listings receive more reviews (bookings) when they are: - More affordable, - Cleaner, - Have more bedrooms, - And are listed longer.

Room type also matters—private rooms perform slightly better than shared rooms in terms of review volume. Hosts looking to improve booking outcomes should focus on cleanliness, pricing, and overall presentation to prospective guests.