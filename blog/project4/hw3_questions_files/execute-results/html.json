{
  "hash": "afc87326bed6865f966021b3333184b9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multinomial Logit Model\"\nauthor: Ouwen Jia\ndate: today\n---\n\nThis assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm.\n\n## 1. Likelihood for the Multi-nomial Logit (MNL) Model\n\nSuppose we have $i=1,\\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \\in \\{1, \\ldots, J\\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.).\n\nWe model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:\n\n$$ U_{ij} = x_j'\\beta + \\epsilon_{ij} $$\n\nwhere $\\epsilon_{ij}$ is an i.i.d. extreme value error term.\n\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:\n\n$$ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} $$\n\nFor example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:\n\n$$ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} $$\n\nA clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\\delta_{ij}$) that indicates the chosen product:\n\n$$ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}$$\n\nNotice that if the consumer selected product $j=3$, then $\\delta_{i3}=1$ while $\\delta_{i1}=\\delta_{i2}=0$ and the likelihood is:\n\n$$ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} $$\n\nThe joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:\n\n$$ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} $$\n\nAnd the joint log-likelihood function is:\n\n$$ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) $$\n\n## 2. Simulate Conjoint Data\n\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a \"no choice\" option; each simulated respondent must select one of the 3 alternatives.\n\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \\$4 to \\$32 in increments of \\$4.\n\nThe part-worths (i.e., preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included advertisements (0 for ad-free); and -0.1\\*price so that utility to consumer $i$ for hypothetical streaming service $j$ is\n\n$$\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n$$\n\nwhere the variables are binary indicators and $\\varepsilon$ is Type 1 Extreme Value (i.e., Gumble) distributed.\n\nThe following code provides the simulation of the conjoint data.\n\n::: {.callout-note collapse=\"true\"}\n::: {.cell}\n\n```{.r .cell-code}\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand <- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad <- c(\"Yes\", \"No\")\nprice <- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles <- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm <- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util <- c(N = 1.0, P = 0.5, H = 0)\na_util <- c(Yes = -0.8, No = 0.0)\np_util <- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps <- 100\nn_tasks <- 10\nn_alts <- 3\n\n# function to simulate one respondent’s data\nsim_one <- function(id) {\n  \n    datlist <- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat <- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v <- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |> round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e <- -log(-log(runif(n_alts)))\n        dat$u <- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice <- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] <- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data <- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data <- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))\n```\n:::\n:::\n\n## 3. Preparing the Data for Estimation\n\nThe \"hard part\" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariant $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariant $k$). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load necessary package\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n\n```{.r .cell-code}\n# Convert brand and ad into dummy variables\n# Use \"Hulu\" and \"Ad-Free\" as base levels (i.e., drop them)\nconjoint_prepped <- conjoint_data %>%\n  mutate(\n    brand_N = ifelse(brand == \"N\", 1, 0),\n    brand_P = ifelse(brand == \"P\", 1, 0),\n    ad_yes  = ifelse(ad == \"Yes\", 1, 0)\n  ) %>%\n  select(resp, task, choice, brand_N, brand_P, ad_yes, price)\n```\n:::\n\n## 4. Estimation via Maximum Likelihood\n\n::: {.cell}\n\n```{.r .cell-code}\nloglik_mnl <- function(par, data) {\n  b_n <- par[1]\n  b_p <- par[2]\n  b_ad <- par[3]\n  b_price <- par[4]\n\n  data$util <- with(data,\n                    b_n * brand_N +\n                    b_p * brand_P +\n                    b_ad * ad_yes +\n                    b_price * price)\n\n  data <- data %>%\n    group_by(resp, task) %>%\n    mutate(\n      exp_util = exp(util),\n      prob = exp_util / sum(exp_util)\n    ) %>%\n    ungroup()\n\n  ll <- sum(log(data$prob[data$choice == 1]))\n  return(-ll)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ninit_par <- c(0, 0, 0, 0)\n\nresult <- optim(par = init_par,\n                fn = loglik_mnl,\n                data = conjoint_prepped,\n                method = \"BFGS\",\n                hessian = TRUE)\n\nestimates <- result$par\nnames(estimates) <- c(\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\")\nestimates\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nbeta_netflix   beta_prime     beta_ads   beta_price \n  0.94120473   0.50161701  -0.73200143  -0.09948157 \n```\n\n\n:::\n:::\n\n### Model Estimation Results\n\nWe estimate a multinomial logit (MNL) model using maximum likelihood. The parameters reflect how each attribute affects utility and choice probability. Netflix has the highest preference weight (β ≈ 1), followed by Prime (β ≈ 0.5), while ads significantly decrease utility (β ≈ -0.8). Price has a negative linear effect on utility, with an estimated coefficient close to -0.1 per dollar.\n\nThe table below shows the point estimates, standard errors, and 95% confidence intervals.\n\n::: {.cell}\n\n```{.r .cell-code}\nvcov <- solve(result$hessian)\nse <- sqrt(diag(vcov))\n\nci <- data.frame(\n  Parameter = names(estimates),\n  Estimate = estimates,\n  SE = se,\n  Lower_95 = estimates - 1.96 * se,\n  Upper_95 = estimates + 1.96 * se\n)\n\nknitr::kable(ci, digits = 3, caption = \"Estimated Parameters with 95% Confidence Intervals\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Estimated Parameters with 95% Confidence Intervals\n\n|             |Parameter    | Estimate|    SE| Lower_95| Upper_95|\n|:------------|:------------|--------:|-----:|--------:|--------:|\n|beta_netflix |beta_netflix |    0.941| 0.111|    0.724|    1.159|\n|beta_prime   |beta_prime   |    0.502| 0.111|    0.284|    0.719|\n|beta_ads     |beta_ads     |   -0.732| 0.088|   -0.904|   -0.560|\n|beta_price   |beta_price   |   -0.099| 0.006|   -0.112|   -0.087|\n\n\n:::\n:::\n\n## 5. Estimation via Bayesian Methods\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set up\nset.seed(123)\nn_samples <- 11000\nburn_in <- 1000\nposterior_draws <- matrix(NA, nrow = n_samples, ncol = 4)\ncolnames(posterior_draws) <- c(\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\")\ncurrent <- c(1.0, 0.5, -0.8, -0.1)\n\n# Proposal SDs: 0.05 for binary var betas, 0.005 for price beta\nproposal_sds <- c(0.05, 0.05, 0.05, 0.005)\n\n# Prior SDs: 5 for binary betas, 1 for price\nprior_sds <- c(5, 5, 5, 1)\n\n# Log-prior\nlog_prior <- function(beta) {\n  sum(dnorm(beta, mean = 0, sd = prior_sds, log = TRUE))\n}\n\n# Log-likelihood (reuse your earlier log-likelihood function)\nlog_post <- function(beta, data) {\n  -loglik_mnl(beta, data) + log_prior(beta)\n}\n\n# Metropolis-Hastings sampler\nposterior_draws[1, ] <- current\nfor (i in 2:n_samples) {\n  proposal <- current + rnorm(4, mean = 0, sd = proposal_sds)\n  log_alpha <- log_post(proposal, conjoint_prepped) - log_post(current, conjoint_prepped)\n  if (log(runif(1)) < log_alpha) {\n    current <- proposal\n  }\n  posterior_draws[i, ] <- current\n}\n\n# Drop burn-in\nposterior_draws <- posterior_draws[(burn_in + 1):n_samples, ]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 2))\nplot(posterior_draws[, \"beta_netflix\"], type = \"l\", main = \"Trace Plot: beta_netflix\",\n     xlab = \"Iteration\", ylab = \"Value\")\nhist(posterior_draws[, \"beta_netflix\"], breaks = 30, main = \"Posterior: beta_netflix\",\n     xlab = \"Value\", freq = FALSE)\n```\n\n::: {.cell-output-display}\n![](hw3_questions_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_summary <- data.frame(\n  Parameter = colnames(posterior_draws),\n  Mean = colMeans(posterior_draws),\n  SD = apply(posterior_draws, 2, sd),\n  Lower_95 = apply(posterior_draws, 2, quantile, probs = 0.025),\n  Upper_95 = apply(posterior_draws, 2, quantile, probs = 0.975)\n)\n\nknitr::kable(posterior_summary, digits = 3, caption = \"Posterior Means, SDs, and 95% Credible Intervals\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Posterior Means, SDs, and 95% Credible Intervals\n\n|             |Parameter    |   Mean|    SD| Lower_95| Upper_95|\n|:------------|:------------|------:|-----:|--------:|--------:|\n|beta_netflix |beta_netflix |  0.929| 0.109|    0.723|    1.147|\n|beta_prime   |beta_prime   |  0.491| 0.112|    0.277|    0.717|\n|beta_ads     |beta_ads     | -0.735| 0.087|   -0.915|   -0.563|\n|beta_price   |beta_price   | -0.100| 0.006|   -0.111|   -0.087|\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncomparison <- posterior_summary\ncomparison$MLE_Estimate <- c(1.00, 0.50, -0.80, -0.10)  # replace with real MLEs\ncomparison$Difference <- comparison$Mean - comparison$MLE_Estimate\n\nknitr::kable(comparison, digits = 3, caption = \"Bayesian vs MLE Estimates\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Bayesian vs MLE Estimates\n\n|             |Parameter    |   Mean|    SD| Lower_95| Upper_95| MLE_Estimate| Difference|\n|:------------|:------------|------:|-----:|--------:|--------:|------------:|----------:|\n|beta_netflix |beta_netflix |  0.929| 0.109|    0.723|    1.147|          1.0|     -0.071|\n|beta_prime   |beta_prime   |  0.491| 0.112|    0.277|    0.717|          0.5|     -0.009|\n|beta_ads     |beta_ads     | -0.735| 0.087|   -0.915|   -0.563|         -0.8|      0.065|\n|beta_price   |beta_price   | -0.100| 0.006|   -0.111|   -0.087|         -0.1|      0.000|\n\n\n:::\n:::\n\n## 6. Discussion\n\nIf this were real-world data rather than a simulation, the estimated parameters would still provide meaningful insight into average consumer preferences. The fact that $\\beta_{\\text{Netflix}} > \\beta_{\\text{Prime}}$ indicates that, holding price and ad presence constant, consumers on average prefer Netflix over Amazon Prime, and both over Hulu (which is the omitted base category). This ranking aligns with expectations based on brand equity, original content, and perceived value.\n\nIt also makes intuitive sense that $\\beta_{\\text{price}}$ is negative. This implies that, all else equal, higher prices reduce the utility of a streaming option, decreasing the likelihood of it being selected. The magnitude and sign of this coefficient capture the population's overall price sensitivity in a straightforward, linear way.\n\n### 📈 Toward a Multi-Level Model\n\nThe model we estimated assumes that all consumers share identical preferences. While this works for simulated data or quick insights, real-world choice data often reveal substantial variation across individuals. For example, some consumers may strongly dislike ads, while others tolerate them for a lower price; some may be highly price-sensitive, while others are not.\n\nTo account for this, we would use a **hierarchical (or multi-level) model**, in which each respondent $i$ has their own vector of part-worths, ${\\beta}_i$. These individual-level coefficients are assumed to be drawn from a population distribution:\n\n$$\n{\\beta}_i \\sim \\mathcal{N}({\\mu}, \\Sigma)\n$$\n\nThis approach allows us to estimate both the population-level average preferences ($\\{\\mu}$) and the variation across individuals ($\\Sigma$).\n\nTo simulate data from a hierarchical model: - We would first draw ${\\beta}_i$ for each respondent from the above distribution. - Then, we would simulate each respondent’s choices using their personalized coefficients.\n\nTo estimate this kind of model, we would rely on Bayesian methods (e.g., using `rstan`, `brms`, or `PyMC`) or simulated maximum likelihood. Hierarchical models are the standard in real-world conjoint analysis because they better reflect the richness and heterogeneity of consumer behavior.",
    "supporting": [
      "hw3_questions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}