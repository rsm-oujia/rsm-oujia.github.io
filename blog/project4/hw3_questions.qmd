---
title: "Multinomial Logit Model"
author: "Ouwen Jia"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
editor: visual
---

This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm.

## 1. Likelihood for the Multi-nomial Logit (MNL) Model

Suppose we have $i=1,\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \in \{1, \ldots, J\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.).

We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:

$$ U_{ij} = x_j'\beta + \epsilon_{ij} $$

where $\epsilon_{ij}$ is an i.i.d. extreme value error term.

The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:

$$ \mathbb{P}_i(j) = \frac{e^{x_j'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:

$$ \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{e^{x_1'\beta} + e^{x_2'\beta} + e^{x_3'\beta}} $$

A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\delta_{ij}$) that indicates the chosen product:

$$ L_i(\beta) = \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} = \mathbb{P}_i(1)^{\delta_{i1}} \times \ldots \times \mathbb{P}_i(J)^{\delta_{iJ}}$$

Notice that if the consumer selected product $j=3$, then $\delta_{i3}=1$ while $\delta_{i1}=\delta_{i2}=0$ and the likelihood is:

$$ L_i(\beta) = \mathbb{P}_i(1)^0 \times \mathbb{P}_i(2)^0 \times \mathbb{P}_i(3)^1 = \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{\sum_{k=1}^3e^{x_k'\beta}} $$

The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:

$$ L_n(\beta) = \prod_{i=1}^n L_i(\beta) = \prod_{i=1}^n \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} $$

And the joint log-likelihood function is:

$$ \ell_n(\beta) = \sum_{i=1}^n \sum_{j=1}^J \delta_{ij} \log(\mathbb{P}_i(j)) $$

## 2. Simulate Conjoint Data

We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a "no choice" option; each simulated respondent must select one of the 3 alternatives.

Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \$4 to \$32 in increments of \$4.

The part-worths (i.e., preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included advertisements (0 for ad-free); and -0.1\*price so that utility to consumer $i$ for hypothetical streaming service $j$ is

$$
u_{ij} = (1 \times Netflix_j) + (0.5 \times Prime_j) + (-0.8*Ads_j) - 0.1\times Price_j + \varepsilon_{ij}
$$

where the variables are binary indicators and $\varepsilon$ is Type 1 Extreme Value (i.e., Gumble) distributed.

The following code provides the simulation of the conjoint data.

::: {.callout-note collapse="true"}
```{r}
# set seed for reproducibility
set.seed(123)

# define attributes
brand <- c("N", "P", "H") # Netflix, Prime, Hulu
ad <- c("Yes", "No")
price <- seq(8, 32, by=4)

# generate all possible profiles
profiles <- expand.grid(
    brand = brand,
    ad = ad,
    price = price
)
m <- nrow(profiles)

# assign part-worth utilities (true parameters)
b_util <- c(N = 1.0, P = 0.5, H = 0)
a_util <- c(Yes = -0.8, No = 0.0)
p_util <- function(p) -0.1 * p

# number of respondents, choice tasks, and alternatives per task
n_peeps <- 100
n_tasks <- 10
n_alts <- 3

# function to simulate one respondentâ€™s data
sim_one <- function(id) {
  
    datlist <- list()
    
    # loop over choice tasks
    for (t in 1:n_tasks) {
        
        # randomly sample 3 alts (better practice would be to use a design)
        dat <- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])
        
        # compute deterministic portion of utility
        dat$v <- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |> round(10)
        
        # add Gumbel noise (Type I extreme value)
        dat$e <- -log(-log(runif(n_alts)))
        dat$u <- dat$v + dat$e
        
        # identify chosen alternative
        dat$choice <- as.integer(dat$u == max(dat$u))
        
        # store task
        datlist[[t]] <- dat
    }
    
    # combine all tasks for one respondent
    do.call(rbind, datlist)
}

# simulate data for all respondents
conjoint_data <- do.call(rbind, lapply(1:n_peeps, sim_one))

# remove values unobservable to the researcher
conjoint_data <- conjoint_data[ , c("resp", "task", "brand", "ad", "price", "choice")]

# clean up
rm(list=setdiff(ls(), "conjoint_data"))
```
:::

## 3. Preparing the Data for Estimation

The "hard part" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariant $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariant $k$). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.

```{r}
# Load necessary package
library(dplyr)

# Convert brand and ad into dummy variables
# Use "Hulu" and "Ad-Free" as base levels (i.e., drop them)
conjoint_prepped <- conjoint_data %>%
  mutate(
    brand_N = ifelse(brand == "N", 1, 0),
    brand_P = ifelse(brand == "P", 1, 0),
    ad_yes  = ifelse(ad == "Yes", 1, 0)
  ) %>%
  select(resp, task, choice, brand_N, brand_P, ad_yes, price)

```

## 4. Estimation via Maximum Likelihood

```{r}
loglik_mnl <- function(par, data) {
  b_n <- par[1]
  b_p <- par[2]
  b_ad <- par[3]
  b_price <- par[4]

  data$util <- with(data,
                    b_n * brand_N +
                    b_p * brand_P +
                    b_ad * ad_yes +
                    b_price * price)

  data <- data %>%
    group_by(resp, task) %>%
    mutate(
      exp_util = exp(util),
      prob = exp_util / sum(exp_util)
    ) %>%
    ungroup()

  ll <- sum(log(data$prob[data$choice == 1]))
  return(-ll)
}

```

```{r}
init_par <- c(0, 0, 0, 0)

result <- optim(par = init_par,
                fn = loglik_mnl,
                data = conjoint_prepped,
                method = "BFGS",
                hessian = TRUE)

estimates <- result$par
names(estimates) <- c("beta_netflix", "beta_prime", "beta_ads", "beta_price")
estimates
```

### Model Estimation Results

We estimate a multinomial logit (MNL) model using maximum likelihood. The parameters reflect how each attribute affects utility and choice probability. Netflix has the highest preference weight (Î² â‰ˆ 1), followed by Prime (Î² â‰ˆ 0.5), while ads significantly decrease utility (Î² â‰ˆ -0.8). Price has a negative linear effect on utility, with an estimated coefficient close to -0.1 per dollar.

The table below shows the point estimates, standard errors, and 95% confidence intervals.

```{r}
vcov <- solve(result$hessian)
se <- sqrt(diag(vcov))

ci <- data.frame(
  Parameter = names(estimates),
  Estimate = estimates,
  SE = se,
  Lower_95 = estimates - 1.96 * se,
  Upper_95 = estimates + 1.96 * se
)

knitr::kable(ci, digits = 3, caption = "Estimated Parameters with 95% Confidence Intervals")

```

## 5. Estimation via Bayesian Methods

```{r}
# Set up
set.seed(123)
n_samples <- 11000
burn_in <- 1000
posterior_draws <- matrix(NA, nrow = n_samples, ncol = 4)
colnames(posterior_draws) <- c("beta_netflix", "beta_prime", "beta_ads", "beta_price")
current <- c(1.0, 0.5, -0.8, -0.1)

# Proposal SDs: 0.05 for binary var betas, 0.005 for price beta
proposal_sds <- c(0.05, 0.05, 0.05, 0.005)

# Prior SDs: 5 for binary betas, 1 for price
prior_sds <- c(5, 5, 5, 1)

# Log-prior
log_prior <- function(beta) {
  sum(dnorm(beta, mean = 0, sd = prior_sds, log = TRUE))
}

# Log-likelihood (reuse your earlier log-likelihood function)
log_post <- function(beta, data) {
  -loglik_mnl(beta, data) + log_prior(beta)
}

# Metropolis-Hastings sampler
posterior_draws[1, ] <- current
for (i in 2:n_samples) {
  proposal <- current + rnorm(4, mean = 0, sd = proposal_sds)
  log_alpha <- log_post(proposal, conjoint_prepped) - log_post(current, conjoint_prepped)
  if (log(runif(1)) < log_alpha) {
    current <- proposal
  }
  posterior_draws[i, ] <- current
}

# Drop burn-in
posterior_draws <- posterior_draws[(burn_in + 1):n_samples, ]

```

```{r}
par(mfrow = c(1, 2))
plot(posterior_draws[, "beta_netflix"], type = "l", main = "Trace Plot: beta_netflix",
     xlab = "Iteration", ylab = "Value")
hist(posterior_draws[, "beta_netflix"], breaks = 30, main = "Posterior: beta_netflix",
     xlab = "Value", freq = FALSE)
```

```{r}
posterior_summary <- data.frame(
  Parameter = colnames(posterior_draws),
  Mean = colMeans(posterior_draws),
  SD = apply(posterior_draws, 2, sd),
  Lower_95 = apply(posterior_draws, 2, quantile, probs = 0.025),
  Upper_95 = apply(posterior_draws, 2, quantile, probs = 0.975)
)

knitr::kable(posterior_summary, digits = 3, caption = "Posterior Means, SDs, and 95% Credible Intervals")
```

```{r}
comparison <- posterior_summary
comparison$MLE_Estimate <- c(1.00, 0.50, -0.80, -0.10)  # replace with real MLEs
comparison$Difference <- comparison$Mean - comparison$MLE_Estimate

knitr::kable(comparison, digits = 3, caption = "Bayesian vs MLE Estimates")
```

## 6. Discussion

If this were real-world data rather than a simulation, the estimated parameters would still provide meaningful insight into average consumer preferences. The fact that $\beta_{\text{Netflix}} > \beta_{\text{Prime}}$ indicates that, holding price and ad presence constant, consumers on average prefer Netflix over Amazon Prime, and both over Hulu (which is the omitted base category). This ranking aligns with expectations based on brand equity, original content, and perceived value.

It also makes intuitive sense that $\beta_{\text{price}}$ is negative. This implies that, all else equal, higher prices reduce the utility of a streaming option, decreasing the likelihood of it being selected. The magnitude and sign of this coefficient capture the population's overall price sensitivity in a straightforward, linear way.

### ðŸ“ˆ Toward a Multi-Level Model

The model we estimated assumes that all consumers share identical preferences. While this works for simulated data or quick insights, real-world choice data often reveal substantial variation across individuals. For example, some consumers may strongly dislike ads, while others tolerate them for a lower price; some may be highly price-sensitive, while others are not.

To account for this, we would use a **hierarchical (or multi-level) model**, in which each respondent $i$ has their own vector of part-worths, ${\beta}_i$. These individual-level coefficients are assumed to be drawn from a population distribution:

$$
{\beta}_i \sim \mathcal{N}({\mu}, \Sigma)
$$

This approach allows us to estimate both the population-level average preferences ($\{\mu}$) and the variation across individuals ($\Sigma$).

To simulate data from a hierarchical model: - We would first draw ${\beta}_i$ for each respondent from the above distribution. - Then, we would simulate each respondentâ€™s choices using their personalized coefficients.

To estimate this kind of model, we would rely on Bayesian methods (e.g., using `rstan`, `brms`, or `PyMC`) or simulated maximum likelihood. Hierarchical models are the standard in real-world conjoint analysis because they better reflect the richness and heterogeneity of consumer behavior.