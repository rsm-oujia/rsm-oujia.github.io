---
title: "HW4 Machine Learning"
author: "Ouwen Jia"
date: today
---

## 1a. K-Means

## Dataset

We use the Palmer Penguins dataset, focusing on **bill_length_mm** and **flipper_length_mm**.

```{r}
library(tidyverse)
library(cluster)
library(ggplot2)
library(gganimate)
library(ggthemes)

# Load data
penguins <- read_csv("blog/project5/palmer_penguins.csv")
df <- penguins %>% 
  select(bill_length_mm, flipper_length_mm) %>% 
  drop_na()


```

## 2a. K Nearest Neighbors

*todo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm. The code generates a dataset with two features, `x1` and `x2`, and a binary outcome variable `y` that is determined by whether `x2` is above or below a wiggly boundary defined by a sin function.*

```{r}
# gen data -----
set.seed(42)
n <- 100
x1 <- runif(n, -3, 3)
x2 <- runif(n, -3, 3)
x <- cbind(x1, x2)

# define a wiggly boundary
boundary <- sin(4*x1) + x1
y <- ifelse(x2 > boundary, 1, 0) |> as.factor()
dat <- data.frame(x1 = x1, x2 = x2, y = y)
```

*todo: plot the data where the horizontal axis is `x1`, the vertical axis is `x2`, and the points are colored by the value of `y`. You may optionally draw the wiggly boundary.*

*todo: generate a test dataset with 100 points, using the same code as above but with a different seed.*

*todo: implement KNN by hand. Check you work with a built-in function -- eg, `class::knn()` or `caret::train(method="knn")` in R, or scikit-learn's `KNeighborsClassifier` in Python.*

*todo: run your function for k=1,...,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?*