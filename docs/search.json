[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ouwen Jia",
    "section": "",
    "text": "Hi this is Owen, welcome to my website."
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data\n\n\n\nI analyzed the data\n#| message: false\nlibrary(tidyverse) mtcars |&gt; ggplot(aes(x = wt, y = mpg)) + geom_point()"
  },
  {
    "objectID": "blog/project1/index.html#section-1-data",
    "href": "blog/project1/index.html#section-1-data",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project1/index.html#section-2-analysis",
    "href": "blog/project1/index.html#section-2-analysis",
    "title": "This is Project 1",
    "section": "",
    "text": "I analyzed the data\n#| message: false\nlibrary(tidyverse) mtcars |&gt; ggplot(aes(x = wt, y = mpg)) + geom_point()"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Matching Donations and Charitable Giving: A Replication Study\n\n\n\n\nOuwen Jia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOuwen Jia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\nOuwen Jia\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\nOuwen Jia\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW4 Machine Learning\n\n\n\n\nOuwen Jia\nJun 11, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "Ouwen Jia",
    "section": "",
    "text": "Hi this is Owen, welcome to my website."
  },
  {
    "objectID": "blog/project1/project1_index.html",
    "href": "blog/project1/project1_index.html",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data\n\n\n\nI analyzed the data\n#| message: false\nlibrary(tidyverse) mtcars |&gt; ggplot(aes(x = wt, y = mpg)) + geom_point()"
  },
  {
    "objectID": "blog/project1/project1_index.html#section-1-data",
    "href": "blog/project1/project1_index.html#section-1-data",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project1/project1_index.html#section-2-analysis",
    "href": "blog/project1/project1_index.html#section-2-analysis",
    "title": "This is Project 1",
    "section": "",
    "text": "I analyzed the data\n#| message: false\nlibrary(tidyverse) mtcars |&gt; ggplot(aes(x = wt, y = mpg)) + geom_point()"
  },
  {
    "objectID": "blog/project2/project2_index.html",
    "href": "blog/project2/project2_index.html",
    "title": "Ouwen Jia",
    "section": "",
    "text": "Hi this is Owen, welcome to my website."
  },
  {
    "objectID": "blog/project3/hw2_questions.html",
    "href": "blog/project3/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\nggplot(data, aes(x = patents, fill = iscustomer)) +\n  geom_histogram(position = \"identity\", alpha = 0.5, bins = 30) +\n  labs(\n    title = \"Number of Patents by Customer Status\",\n    x = \"Patents (Last 5 Years)\",\n    fill = \"Customer Status\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ndata %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(\n    count = n(),\n    mean_patents = mean(patents),\n    sd_patents = sd(patents)\n  )\n\n# A tibble: 2 × 4\n  iscustomer   count mean_patents sd_patents\n  &lt;fct&gt;        &lt;int&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 Non-Customer  1019         3.47       2.23\n2 Customer       481         4.13       2.55\n\n\nWe observe that Blueprinty customers, on average, have more patents over the past five years. However, this comparison does not yet account for potential confounding variables.\n\n\n\n\nggplot(data, aes(x = region, fill = iscustomer)) +\n  geom_bar(position = \"fill\") +\n  labs(\n    title = \"Regional Distribution by Customer Status\",\n    x = \"Region\",\n    y = \"Proportion of Firms\",\n    fill = \"Customer Status\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere are noticeable regional differences in customer adoption. Some regions appear to have a higher share of Blueprinty customers.\n\n\n\n\nggplot(data, aes(x = age, fill = iscustomer)) +\n  geom_histogram(position = \"identity\", alpha = 0.5, bins = 30) +\n  labs(\n    title = \"Firm Age Distribution by Customer Status\",\n    x = \"Age (Years Since Incorporation)\",\n    fill = \"Customer Status\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ndata %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(\n    mean_age = mean(age),\n    sd_age = sd(age)\n  )\n\n# A tibble: 2 × 3\n  iscustomer   mean_age sd_age\n  &lt;fct&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1 Non-Customer     26.1   6.95\n2 Customer         26.9   7.81\n\n\nOn average, customers are older firms. Age may therefore be a confounding factor when evaluating patent success.\n\n\n\nWhile customers have more patents on average, they also differ in age and region. These structural differences suggest we should adjust for confounding variables in any attempt to infer a causal impact of Blueprinty’s software.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe probability mass function for a Poisson distribution is:\n\\(f(Y | \\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\\)\nWe treat the observed number of patents as realizations from this distribution.\n\n\n\n# Log-likelihood function for Poisson model\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) return(-Inf)  # Poisson lambda must be positive\n  sum(dpois(Y, lambda, log = TRUE))\n}\n\n\n\n\nWe use the observed number of patents as input to our log-likelihood function.\n\nY_obs &lt;- data$patents\nlambda_vals &lt;- seq(0.1, 10, by = 0.1)\nloglik_vals &lt;- sapply(lambda_vals, function(l) poisson_loglikelihood(l, Y_obs))\n\nplot(lambda_vals, loglik_vals, type = \"l\", lwd = 2,\n     main = \"Log-Likelihood of Poisson Model\",\n     xlab = expression(lambda), ylab = \"Log-Likelihood\")\n\n\n\n\n\n\n\n\n\n\n\nWe now use optim() to find the value of lambda that maximizes the log-likelihood.\n\nmle_result &lt;- optim(par = 1, fn = function(l) -poisson_loglikelihood(l, Y_obs),\n                    method = \"Brent\", lower = 0.01, upper = 20)\n\nlambda_mle &lt;- mle_result$par\nlambda_mle\n\n[1] 3.684667\n\n\nThe MLE for \\(\\lambda\\) is the sample mean of \\(Y\\), which is consistent with theory: for Poisson-distributed data, \\(\\hat{\\lambda}_{MLE} = \\bar{Y}\\).\n\nmean(Y_obs)\n\n[1] 3.684667\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n\npoisson_regression_loglik &lt;- function(beta, Y, X) {\n  lambda &lt;- exp(X %*% beta)\n  sum(dpois(Y, lambda, log = TRUE))\n}\n\n\n\n\n\n# Age and age squared\ndata &lt;- data %&gt;%\n  mutate(age_sq = age^2)\n\n# One-hot encode region (drop 1 category)\nregion_dummies &lt;- model.matrix(~ region, data = data)[, -1]\n\n# Final covariate matrix: intercept, age, age_sq, region dummies, customer\nX &lt;- cbind(\n  intercept = 1,\n  age = data$age,\n  age_sq = data$age_sq,\n  region_dummies,\n  iscustomer = as.numeric(data$iscustomer == \"Customer\")\n)\n\nY &lt;- data$patents\n\n\n\n\n\nloglik_wrapper &lt;- function(beta) -poisson_regression_loglik(beta, Y, X)\n\n# Initial guess\nbeta_init &lt;- rep(0, ncol(X))\n\n# Optimize\nopt_result &lt;- optim(par = beta_init, fn = loglik_wrapper, hessian = TRUE, method = \"BFGS\")\n\n# Coefficients\nbeta_hat &lt;- opt_result$par\n\n# Standard errors from Hessian\nhess_inv &lt;- solve(opt_result$hessian)\nse_hat &lt;- sqrt(diag(hess_inv))\n\n# Summary table\ncoef_table &lt;- data.frame(\n  Term = colnames(X),\n  Estimate = beta_hat,\n  StdError = se_hat\n)\n\ncoef_table\n\n             Term     Estimate     StdError\n1       intercept -0.125735914 0.1122180345\n2             age  0.115793715 0.0063574229\n3          age_sq -0.002228748 0.0000771291\n4 regionNortheast -0.024556782 0.0433762879\n5 regionNorthwest -0.034827790 0.0529311002\n6     regionSouth -0.005441860 0.0524007440\n7 regionSouthwest -0.037784109 0.0471722463\n8      iscustomer  0.060665584 0.0320588299\n\n\n\n\n\n\nglm_model &lt;- glm(patents ~ age + I(age^2) + region + iscustomer,\n                 family = poisson(link = \"log\"),\n                 data = data)\n\nsummary(glm_model)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(link = \"log\"), data = data)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        -0.508920   0.183179  -2.778  0.00546 ** \nage                 0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)           -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast     0.029170   0.043625   0.669  0.50372    \nregionNorthwest    -0.017574   0.053781  -0.327  0.74383    \nregionSouth         0.056561   0.052662   1.074  0.28281    \nregionSouthwest     0.050576   0.047198   1.072  0.28391    \niscustomerCustomer  0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n\nThis Poisson regression estimates the expected number of patents awarded to firms as a function of age, age squared, region, and Blueprinty software usage. Key insights include:\n\nage: The coefficient is positive and highly significant. This suggests that older firms tend to have more patents, all else equal. Specifically, a one-year increase in firm age is associated with an expected increase in patents by a factor of exp(0.1486) ≈ 1.160 (or about a 16% increase).\nI(age²): The negative and significant coefficient on age squared indicates a concave (inverted U-shaped) relationship. This means that the rate of increase in patents slows down for very old firms.\nregion: None of the regional dummy variables (Northeast, Northwest, South, Southwest) are statistically significant. This implies that — after controlling for age and Blueprinty usage — there are no strong regional differences in patent counts.\niscustomerCustomer: The coefficient is positive and highly significant (p &lt; 0.001). Using Blueprinty’s software is associated with an increase in the expected number of patents. The estimated coefficient 0.208 corresponds to a 23.1% increase in expected patent count (exp(0.208) ≈ 1.231).\n\n\n\n\n\n# Create X_0 and X_1 matrices\nX_0 &lt;- X\nX_0[, \"iscustomer\"] &lt;- 0\n\nX_1 &lt;- X\nX_1[, \"iscustomer\"] &lt;- 1\n\n# Predicted lambda for each case\nlambda_0 &lt;- exp(X_0 %*% beta_hat)\nlambda_1 &lt;- exp(X_1 %*% beta_hat)\n\n# Predicted difference\ndiff &lt;- lambda_1 - lambda_0\nmean(diff)\n\n[1] 0.2178843\n\n\nThis gives the average expected increase in the number of patents due to using Blueprinty’s software, across all firms in the dataset."
  },
  {
    "objectID": "blog/project3/hw2_questions.html#blueprinty-case-study",
    "href": "blog/project3/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\nggplot(data, aes(x = patents, fill = iscustomer)) +\n  geom_histogram(position = \"identity\", alpha = 0.5, bins = 30) +\n  labs(\n    title = \"Number of Patents by Customer Status\",\n    x = \"Patents (Last 5 Years)\",\n    fill = \"Customer Status\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ndata %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(\n    count = n(),\n    mean_patents = mean(patents),\n    sd_patents = sd(patents)\n  )\n\n# A tibble: 2 × 4\n  iscustomer   count mean_patents sd_patents\n  &lt;fct&gt;        &lt;int&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 Non-Customer  1019         3.47       2.23\n2 Customer       481         4.13       2.55\n\n\nWe observe that Blueprinty customers, on average, have more patents over the past five years. However, this comparison does not yet account for potential confounding variables.\n\n\n\n\nggplot(data, aes(x = region, fill = iscustomer)) +\n  geom_bar(position = \"fill\") +\n  labs(\n    title = \"Regional Distribution by Customer Status\",\n    x = \"Region\",\n    y = \"Proportion of Firms\",\n    fill = \"Customer Status\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere are noticeable regional differences in customer adoption. Some regions appear to have a higher share of Blueprinty customers.\n\n\n\n\nggplot(data, aes(x = age, fill = iscustomer)) +\n  geom_histogram(position = \"identity\", alpha = 0.5, bins = 30) +\n  labs(\n    title = \"Firm Age Distribution by Customer Status\",\n    x = \"Age (Years Since Incorporation)\",\n    fill = \"Customer Status\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ndata %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(\n    mean_age = mean(age),\n    sd_age = sd(age)\n  )\n\n# A tibble: 2 × 3\n  iscustomer   mean_age sd_age\n  &lt;fct&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1 Non-Customer     26.1   6.95\n2 Customer         26.9   7.81\n\n\nOn average, customers are older firms. Age may therefore be a confounding factor when evaluating patent success.\n\n\n\nWhile customers have more patents on average, they also differ in age and region. These structural differences suggest we should adjust for confounding variables in any attempt to infer a causal impact of Blueprinty’s software.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe probability mass function for a Poisson distribution is:\n\\(f(Y | \\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\\)\nWe treat the observed number of patents as realizations from this distribution.\n\n\n\n# Log-likelihood function for Poisson model\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) return(-Inf)  # Poisson lambda must be positive\n  sum(dpois(Y, lambda, log = TRUE))\n}\n\n\n\n\nWe use the observed number of patents as input to our log-likelihood function.\n\nY_obs &lt;- data$patents\nlambda_vals &lt;- seq(0.1, 10, by = 0.1)\nloglik_vals &lt;- sapply(lambda_vals, function(l) poisson_loglikelihood(l, Y_obs))\n\nplot(lambda_vals, loglik_vals, type = \"l\", lwd = 2,\n     main = \"Log-Likelihood of Poisson Model\",\n     xlab = expression(lambda), ylab = \"Log-Likelihood\")\n\n\n\n\n\n\n\n\n\n\n\nWe now use optim() to find the value of lambda that maximizes the log-likelihood.\n\nmle_result &lt;- optim(par = 1, fn = function(l) -poisson_loglikelihood(l, Y_obs),\n                    method = \"Brent\", lower = 0.01, upper = 20)\n\nlambda_mle &lt;- mle_result$par\nlambda_mle\n\n[1] 3.684667\n\n\nThe MLE for \\(\\lambda\\) is the sample mean of \\(Y\\), which is consistent with theory: for Poisson-distributed data, \\(\\hat{\\lambda}_{MLE} = \\bar{Y}\\).\n\nmean(Y_obs)\n\n[1] 3.684667\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n\npoisson_regression_loglik &lt;- function(beta, Y, X) {\n  lambda &lt;- exp(X %*% beta)\n  sum(dpois(Y, lambda, log = TRUE))\n}\n\n\n\n\n\n# Age and age squared\ndata &lt;- data %&gt;%\n  mutate(age_sq = age^2)\n\n# One-hot encode region (drop 1 category)\nregion_dummies &lt;- model.matrix(~ region, data = data)[, -1]\n\n# Final covariate matrix: intercept, age, age_sq, region dummies, customer\nX &lt;- cbind(\n  intercept = 1,\n  age = data$age,\n  age_sq = data$age_sq,\n  region_dummies,\n  iscustomer = as.numeric(data$iscustomer == \"Customer\")\n)\n\nY &lt;- data$patents\n\n\n\n\n\nloglik_wrapper &lt;- function(beta) -poisson_regression_loglik(beta, Y, X)\n\n# Initial guess\nbeta_init &lt;- rep(0, ncol(X))\n\n# Optimize\nopt_result &lt;- optim(par = beta_init, fn = loglik_wrapper, hessian = TRUE, method = \"BFGS\")\n\n# Coefficients\nbeta_hat &lt;- opt_result$par\n\n# Standard errors from Hessian\nhess_inv &lt;- solve(opt_result$hessian)\nse_hat &lt;- sqrt(diag(hess_inv))\n\n# Summary table\ncoef_table &lt;- data.frame(\n  Term = colnames(X),\n  Estimate = beta_hat,\n  StdError = se_hat\n)\n\ncoef_table\n\n             Term     Estimate     StdError\n1       intercept -0.125735914 0.1122180345\n2             age  0.115793715 0.0063574229\n3          age_sq -0.002228748 0.0000771291\n4 regionNortheast -0.024556782 0.0433762879\n5 regionNorthwest -0.034827790 0.0529311002\n6     regionSouth -0.005441860 0.0524007440\n7 regionSouthwest -0.037784109 0.0471722463\n8      iscustomer  0.060665584 0.0320588299\n\n\n\n\n\n\nglm_model &lt;- glm(patents ~ age + I(age^2) + region + iscustomer,\n                 family = poisson(link = \"log\"),\n                 data = data)\n\nsummary(glm_model)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(link = \"log\"), data = data)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        -0.508920   0.183179  -2.778  0.00546 ** \nage                 0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)           -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast     0.029170   0.043625   0.669  0.50372    \nregionNorthwest    -0.017574   0.053781  -0.327  0.74383    \nregionSouth         0.056561   0.052662   1.074  0.28281    \nregionSouthwest     0.050576   0.047198   1.072  0.28391    \niscustomerCustomer  0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n\nThis Poisson regression estimates the expected number of patents awarded to firms as a function of age, age squared, region, and Blueprinty software usage. Key insights include:\n\nage: The coefficient is positive and highly significant. This suggests that older firms tend to have more patents, all else equal. Specifically, a one-year increase in firm age is associated with an expected increase in patents by a factor of exp(0.1486) ≈ 1.160 (or about a 16% increase).\nI(age²): The negative and significant coefficient on age squared indicates a concave (inverted U-shaped) relationship. This means that the rate of increase in patents slows down for very old firms.\nregion: None of the regional dummy variables (Northeast, Northwest, South, Southwest) are statistically significant. This implies that — after controlling for age and Blueprinty usage — there are no strong regional differences in patent counts.\niscustomerCustomer: The coefficient is positive and highly significant (p &lt; 0.001). Using Blueprinty’s software is associated with an increase in the expected number of patents. The estimated coefficient 0.208 corresponds to a 23.1% increase in expected patent count (exp(0.208) ≈ 1.231).\n\n\n\n\n\n# Create X_0 and X_1 matrices\nX_0 &lt;- X\nX_0[, \"iscustomer\"] &lt;- 0\n\nX_1 &lt;- X\nX_1[, \"iscustomer\"] &lt;- 1\n\n# Predicted lambda for each case\nlambda_0 &lt;- exp(X_0 %*% beta_hat)\nlambda_1 &lt;- exp(X_1 %*% beta_hat)\n\n# Predicted difference\ndiff &lt;- lambda_1 - lambda_0\nmean(diff)\n\n[1] 0.2178843\n\n\nThis gives the average expected increase in the number of patents due to using Blueprinty’s software, across all firms in the dataset."
  },
  {
    "objectID": "blog/project3/hw2_questions.html#airbnb-case-study",
    "href": "blog/project3/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nExploratory Data Analysis\n\nairbnb &lt;- read_csv(\"airbnb.csv\")\n\nNew names:\nRows: 40628 Columns: 14\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): last_scraped, host_since, room_type dbl (10): ...1, id, days, bathrooms,\nbedrooms, price, number_of_reviews, rev... lgl (1): instant_bookable\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nglimpse(airbnb)\n\nRows: 40,628\nColumns: 14\n$ ...1                      &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1…\n$ id                        &lt;dbl&gt; 2515, 2595, 3647, 3831, 4611, 5099, 5107, 51…\n$ days                      &lt;dbl&gt; 3130, 3127, 3050, 3038, 3012, 2981, 2981, 29…\n$ last_scraped              &lt;chr&gt; \"4/2/2017\", \"4/2/2017\", \"4/2/2017\", \"4/2/201…\n$ host_since                &lt;chr&gt; \"9/6/2008\", \"9/9/2008\", \"11/25/2008\", \"12/7/…\n$ room_type                 &lt;chr&gt; \"Private room\", \"Entire home/apt\", \"Private …\n$ bathrooms                 &lt;dbl&gt; 1, 1, 1, 1, NA, 1, 1, NA, 1, 1, 1, 1, 1, NA,…\n$ bedrooms                  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2,…\n$ price                     &lt;dbl&gt; 59, 230, 150, 89, 39, 212, 250, 60, 129, 79,…\n$ number_of_reviews         &lt;dbl&gt; 150, 20, 0, 116, 93, 60, 60, 50, 53, 329, 11…\n$ review_scores_cleanliness &lt;dbl&gt; 9, 9, NA, 9, 9, 9, 10, 8, 9, 7, 10, 9, 9, 9,…\n$ review_scores_location    &lt;dbl&gt; 9, 10, NA, 9, 8, 9, 9, 9, 10, 10, 10, 9, 10,…\n$ review_scores_value       &lt;dbl&gt; 9, 9, NA, 9, 9, 9, 10, 9, 9, 9, 10, 9, 10, 9…\n$ instant_bookable          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FAL…\n\nsummary(airbnb)\n\n      ...1             id                days       last_scraped      \n Min.   :    1   Min.   :    2515   Min.   :    1   Length:40628      \n 1st Qu.:10158   1st Qu.: 4889868   1st Qu.:  542   Class :character  \n Median :20314   Median : 9862878   Median :  996   Mode  :character  \n Mean   :20314   Mean   : 9698889   Mean   : 1102                     \n 3rd Qu.:30471   3rd Qu.:14667894   3rd Qu.: 1535                     \n Max.   :40628   Max.   :18009669   Max.   :42828                     \n                                                                      \n  host_since         room_type           bathrooms        bedrooms     \n Length:40628       Length:40628       Min.   :0.000   Min.   : 0.000  \n Class :character   Class :character   1st Qu.:1.000   1st Qu.: 1.000  \n Mode  :character   Mode  :character   Median :1.000   Median : 1.000  \n                                       Mean   :1.125   Mean   : 1.147  \n                                       3rd Qu.:1.000   3rd Qu.: 1.000  \n                                       Max.   :8.000   Max.   :10.000  \n                                       NA's   :160     NA's   :76      \n     price         number_of_reviews review_scores_cleanliness\n Min.   :   10.0   Min.   :  0.0     Min.   : 2.000           \n 1st Qu.:   70.0   1st Qu.:  1.0     1st Qu.: 9.000           \n Median :  100.0   Median :  4.0     Median :10.000           \n Mean   :  144.8   Mean   : 15.9     Mean   : 9.198           \n 3rd Qu.:  170.0   3rd Qu.: 17.0     3rd Qu.:10.000           \n Max.   :10000.0   Max.   :421.0     Max.   :10.000           \n                                     NA's   :10195            \n review_scores_location review_scores_value instant_bookable\n Min.   : 2.000         Min.   : 2.000      Mode :logical   \n 1st Qu.: 9.000         1st Qu.: 9.000      FALSE:32759     \n Median :10.000         Median :10.000      TRUE :7869      \n Mean   : 9.414         Mean   : 9.332                      \n 3rd Qu.:10.000         3rd Qu.:10.000                      \n Max.   :10.000         Max.   :10.000                      \n NA's   :10254          NA's   :10256                       \n\n\n\n\nData Cleaning\n\n# Drop rows with missing values in relevant columns\nairbnb_clean &lt;- airbnb %&gt;%\n  filter(\n    !is.na(number_of_reviews),\n    !is.na(review_scores_cleanliness),\n    !is.na(review_scores_location),\n    !is.na(review_scores_value),\n    !is.na(bathrooms),\n    !is.na(bedrooms),\n    !is.na(price)\n  ) %&gt;%\n  mutate(\n    instant_bookable = ifelse(instant_bookable == \"t\", 1, 0),\n    room_type = as.factor(room_type)\n  )\n\nsummary(airbnb_clean)\n\n      ...1             id                days       last_scraped      \n Min.   :    1   Min.   :    2515   Min.   :    7   Length:30160      \n 1st Qu.: 8630   1st Qu.: 4276690   1st Qu.:  584   Class :character  \n Median :18236   Median : 9149028   Median : 1041   Mode  :character  \n Mean   :18679   Mean   : 8978287   Mean   : 1140                     \n 3rd Qu.:28532   3rd Qu.:13914758   3rd Qu.: 1592                     \n Max.   :40504   Max.   :17973686   Max.   :42828                     \n  host_since                  room_type       bathrooms        bedrooms     \n Length:30160       Entire home/apt:15543   Min.   :0.000   Min.   : 0.000  \n Class :character   Private room   :13773   1st Qu.:1.000   1st Qu.: 1.000  \n Mode  :character   Shared room    :  844   Median :1.000   Median : 1.000  \n                                            Mean   :1.122   Mean   : 1.151  \n                                            3rd Qu.:1.000   3rd Qu.: 1.000  \n                                            Max.   :6.000   Max.   :10.000  \n     price         number_of_reviews review_scores_cleanliness\n Min.   :   10.0   Min.   :  1.00    Min.   : 2.000           \n 1st Qu.:   70.0   1st Qu.:  3.00    1st Qu.: 9.000           \n Median :  103.0   Median :  8.00    Median :10.000           \n Mean   :  140.2   Mean   : 21.17    Mean   : 9.202           \n 3rd Qu.:  169.0   3rd Qu.: 26.00    3rd Qu.:10.000           \n Max.   :10000.0   Max.   :421.00    Max.   :10.000           \n review_scores_location review_scores_value instant_bookable\n Min.   : 2.000         Min.   : 2.000      Min.   :0       \n 1st Qu.: 9.000         1st Qu.: 9.000      1st Qu.:0       \n Median :10.000         Median :10.000      Median :0       \n Mean   : 9.415         Mean   : 9.334      Mean   :0       \n 3rd Qu.:10.000         3rd Qu.:10.000      3rd Qu.:0       \n Max.   :10.000         Max.   :10.000      Max.   :0       \n\n\n\n\nDistribution of Reviews\n\nggplot(airbnb_clean, aes(x = number_of_reviews)) +\n  geom_histogram(bins = 50, fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"Distribution of Number of Reviews\", x = \"Number of Reviews\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Model\n\nmodel &lt;- glm(number_of_reviews ~ days + price + bedrooms + bathrooms +\n               review_scores_cleanliness + review_scores_location +\n               review_scores_value + instant_bookable + room_type,\n             family = poisson(link = \"log\"),\n             data = airbnb_clean)\n\nsummary(model)\n\n\nCall:\nglm(formula = number_of_reviews ~ days + price + bedrooms + bathrooms + \n    review_scores_cleanliness + review_scores_location + review_scores_value + \n    instant_bookable + room_type, family = poisson(link = \"log\"), \n    data = airbnb_clean)\n\nCoefficients: (1 not defined because of singularities)\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                3.646e+00  1.595e-02 228.572  &lt; 2e-16 ***\ndays                       4.962e-05  4.029e-07 123.163  &lt; 2e-16 ***\nprice                     -3.697e-05  8.554e-06  -4.322 1.55e-05 ***\nbedrooms                   7.562e-02  2.005e-03  37.715  &lt; 2e-16 ***\nbathrooms                 -1.105e-01  3.789e-03 -29.163  &lt; 2e-16 ***\nreview_scores_cleanliness  1.138e-01  1.489e-03  76.419  &lt; 2e-16 ***\nreview_scores_location    -8.086e-02  1.600e-03 -50.527  &lt; 2e-16 ***\nreview_scores_value       -9.708e-02  1.795e-03 -54.091  &lt; 2e-16 ***\ninstant_bookable                  NA         NA      NA       NA    \nroom_typePrivate room      1.213e-02  2.735e-03   4.435 9.19e-06 ***\nroom_typeShared room      -2.172e-01  8.616e-03 -25.204  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 961626  on 30159  degrees of freedom\nResidual deviance: 940403  on 30150  degrees of freedom\nAIC: 1061889\n\nNumber of Fisher Scoring iterations: 9\n\n\n\n\nInterpretation\nThe Poisson regression model estimates how various listing characteristics influence the expected number of reviews (used as a proxy for bookings). Below are key takeaways:\n\ndays: Longer-listed properties tend to receive more reviews. Each additional day on the platform increases expected reviews by a factor of exp(4.962e-05) ≈ 1.00005.\nprice: Listings with higher nightly prices receive fewer reviews. The negative and significant coefficient suggests price sensitivity.\nbedrooms: Each additional bedroom increases expected reviews by approximately 7.8% (exp(0.0756) ≈ 1.0785).\nbathrooms: Surprisingly, more bathrooms are associated with fewer reviews, which may reflect multicollinearity or data-specific effects.\nreview_scores_cleanliness: A 1-point increase in cleanliness rating is associated with a 12% increase in expected reviews (exp(0.1138) ≈ 1.12).\nreview_scores_location and review_scores_value: These have negative coefficients, which may suggest correlation with other factors or overcontrol in the model.\nroom_type:\n\nPrivate room: Increases reviews slightly compared to entire homes (exp(0.0213) ≈ 1.0215).\nShared room: Associated with significantly fewer reviews (exp(-0.2172) ≈ 0.805).\n\ninstant_bookable: Dropped from the model due to collinearity, indicating redundancy or lack of variation.\n\n\n\nConclusion\nThe analysis shows that Airbnb listings receive more reviews (bookings) when they are: - More affordable, - Cleaner, - Have more bedrooms, - And are listed longer.\nRoom type also matters—private rooms perform slightly better than shared rooms in terms of review volume. Hosts looking to improve booking outcomes should focus on cleanliness, pricing, and overall presentation to prospective guests."
  },
  {
    "objectID": "blog/project3/blueprinty_case.html",
    "href": "blog/project3/blueprinty_case.html",
    "title": "Blueprinty Case Study",
    "section": "",
    "text": "Blueprinty is a small software firm that develops tools for creating blueprints used in patent applications. The marketing team wants to make the case that firms using Blueprinty’s product are more successful in getting patents approved.\nAlthough we don’t have before-and-after data, Blueprinty has collected cross-sectional data on 1,500 mature engineering firms, including number of patents awarded in the last 5 years, region, firm age, and whether or not the firm uses their software.\n\n\n\n\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\ndata &lt;- read_csv(\"blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata &lt;- data %&gt;%\n  mutate(\n    iscustomer = factor(iscustomer, levels = c(0, 1), labels = c(\"Non-Customer\", \"Customer\"))\n  )\n\n\n\n\nggplot(data, aes(x = patents, fill = iscustomer)) +\n  geom_histogram(position = \"identity\", alpha = 0.5, bins = 30) +\n  labs(\n    title = \"Number of Patents by Customer Status\",\n    x = \"Patents (Last 5 Years)\",\n    fill = \"Customer Status\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ndata %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(\n    count = n(),\n    mean_patents = mean(patents),\n    sd_patents = sd(patents)\n  )\n\n# A tibble: 2 × 4\n  iscustomer   count mean_patents sd_patents\n  &lt;fct&gt;        &lt;int&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 Non-Customer  1019         3.47       2.23\n2 Customer       481         4.13       2.55\n\n\nWe observe that Blueprinty customers, on average, have more patents over the past five years. However, this comparison does not yet account for potential confounding variables.\n\n\n\n\nggplot(data, aes(x = region, fill = iscustomer)) +\n  geom_bar(position = \"fill\") +\n  labs(\n    title = \"Regional Distribution by Customer Status\",\n    x = \"Region\",\n    y = \"Proportion of Firms\",\n    fill = \"Customer Status\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere are noticeable regional differences in customer adoption. Some regions appear to have a higher share of Blueprinty customers.\n\n\n\n\nggplot(data, aes(x = age, fill = iscustomer)) +\n  geom_histogram(position = \"identity\", alpha = 0.5, bins = 30) +\n  labs(\n    title = \"Firm Age Distribution by Customer Status\",\n    x = \"Age (Years Since Incorporation)\",\n    fill = \"Customer Status\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ndata %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(\n    mean_age = mean(age),\n    sd_age = sd(age)\n  )\n\n# A tibble: 2 × 3\n  iscustomer   mean_age sd_age\n  &lt;fct&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1 Non-Customer     26.1   6.95\n2 Customer         26.9   7.81\n\n\nOn average, customers are older firms. Age may therefore be a confounding factor when evaluating patent success.\n\n\n\n\nWhile customers have more patents on average, they also differ in age and region. These structural differences suggest we should adjust for confounding variables in any attempt to infer a causal impact of Blueprinty’s software."
  },
  {
    "objectID": "blog/project3/blueprinty_case.html#blueprinty-case-study",
    "href": "blog/project3/blueprinty_case.html#blueprinty-case-study",
    "title": "Blueprinty Case Study",
    "section": "",
    "text": "Blueprinty is a small software firm that develops tools for creating blueprints used in patent applications. The marketing team wants to make the case that firms using Blueprinty’s product are more successful in getting patents approved.\nAlthough we don’t have before-and-after data, Blueprinty has collected cross-sectional data on 1,500 mature engineering firms, including number of patents awarded in the last 5 years, region, firm age, and whether or not the firm uses their software.\n\n\n\n\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\ndata &lt;- read_csv(\"blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata &lt;- data %&gt;%\n  mutate(\n    iscustomer = factor(iscustomer, levels = c(0, 1), labels = c(\"Non-Customer\", \"Customer\"))\n  )\n\n\n\n\nggplot(data, aes(x = patents, fill = iscustomer)) +\n  geom_histogram(position = \"identity\", alpha = 0.5, bins = 30) +\n  labs(\n    title = \"Number of Patents by Customer Status\",\n    x = \"Patents (Last 5 Years)\",\n    fill = \"Customer Status\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ndata %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(\n    count = n(),\n    mean_patents = mean(patents),\n    sd_patents = sd(patents)\n  )\n\n# A tibble: 2 × 4\n  iscustomer   count mean_patents sd_patents\n  &lt;fct&gt;        &lt;int&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 Non-Customer  1019         3.47       2.23\n2 Customer       481         4.13       2.55\n\n\nWe observe that Blueprinty customers, on average, have more patents over the past five years. However, this comparison does not yet account for potential confounding variables.\n\n\n\n\nggplot(data, aes(x = region, fill = iscustomer)) +\n  geom_bar(position = \"fill\") +\n  labs(\n    title = \"Regional Distribution by Customer Status\",\n    x = \"Region\",\n    y = \"Proportion of Firms\",\n    fill = \"Customer Status\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere are noticeable regional differences in customer adoption. Some regions appear to have a higher share of Blueprinty customers.\n\n\n\n\nggplot(data, aes(x = age, fill = iscustomer)) +\n  geom_histogram(position = \"identity\", alpha = 0.5, bins = 30) +\n  labs(\n    title = \"Firm Age Distribution by Customer Status\",\n    x = \"Age (Years Since Incorporation)\",\n    fill = \"Customer Status\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ndata %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(\n    mean_age = mean(age),\n    sd_age = sd(age)\n  )\n\n# A tibble: 2 × 3\n  iscustomer   mean_age sd_age\n  &lt;fct&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1 Non-Customer     26.1   6.95\n2 Customer         26.9   7.81\n\n\nOn average, customers are older firms. Age may therefore be a confounding factor when evaluating patent success.\n\n\n\n\nWhile customers have more patents on average, they also differ in age and region. These structural differences suggest we should adjust for confounding variables in any attempt to infer a causal impact of Blueprinty’s software."
  },
  {
    "objectID": "blog/project4/hw3_questions.html",
    "href": "blog/project4/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/project4/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/project4/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/project4/hw3_questions.html#simulate-conjoint-data",
    "href": "blog/project4/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (i.e., preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included advertisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothetical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (i.e., Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n  \n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))"
  },
  {
    "objectID": "blog/project4/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "blog/project4/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariant \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariant \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n# Load necessary package\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Convert brand and ad into dummy variables\n# Use \"Hulu\" and \"Ad-Free\" as base levels (i.e., drop them)\nconjoint_prepped &lt;- conjoint_data %&gt;%\n  mutate(\n    brand_N = ifelse(brand == \"N\", 1, 0),\n    brand_P = ifelse(brand == \"P\", 1, 0),\n    ad_yes  = ifelse(ad == \"Yes\", 1, 0)\n  ) %&gt;%\n  select(resp, task, choice, brand_N, brand_P, ad_yes, price)"
  },
  {
    "objectID": "blog/project4/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "blog/project4/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\nloglik_mnl &lt;- function(par, data) {\n  b_n &lt;- par[1]\n  b_p &lt;- par[2]\n  b_ad &lt;- par[3]\n  b_price &lt;- par[4]\n\n  data$util &lt;- with(data,\n                    b_n * brand_N +\n                    b_p * brand_P +\n                    b_ad * ad_yes +\n                    b_price * price)\n\n  data &lt;- data %&gt;%\n    group_by(resp, task) %&gt;%\n    mutate(\n      exp_util = exp(util),\n      prob = exp_util / sum(exp_util)\n    ) %&gt;%\n    ungroup()\n\n  ll &lt;- sum(log(data$prob[data$choice == 1]))\n  return(-ll)\n}\n\n\ninit_par &lt;- c(0, 0, 0, 0)\n\nresult &lt;- optim(par = init_par,\n                fn = loglik_mnl,\n                data = conjoint_prepped,\n                method = \"BFGS\",\n                hessian = TRUE)\n\nestimates &lt;- result$par\nnames(estimates) &lt;- c(\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\")\nestimates\n\nbeta_netflix   beta_prime     beta_ads   beta_price \n  0.94120473   0.50161701  -0.73200143  -0.09948157 \n\n\n\nModel Estimation Results\nWe estimate a multinomial logit (MNL) model using maximum likelihood. The parameters reflect how each attribute affects utility and choice probability. Netflix has the highest preference weight (β ≈ 1), followed by Prime (β ≈ 0.5), while ads significantly decrease utility (β ≈ -0.8). Price has a negative linear effect on utility, with an estimated coefficient close to -0.1 per dollar.\nThe table below shows the point estimates, standard errors, and 95% confidence intervals.\n\nvcov &lt;- solve(result$hessian)\nse &lt;- sqrt(diag(vcov))\n\nci &lt;- data.frame(\n  Parameter = names(estimates),\n  Estimate = estimates,\n  SE = se,\n  Lower_95 = estimates - 1.96 * se,\n  Upper_95 = estimates + 1.96 * se\n)\n\nknitr::kable(ci, digits = 3, caption = \"Estimated Parameters with 95% Confidence Intervals\")\n\n\nEstimated Parameters with 95% Confidence Intervals\n\n\n\nParameter\nEstimate\nSE\nLower_95\nUpper_95\n\n\n\n\nbeta_netflix\nbeta_netflix\n0.941\n0.111\n0.724\n1.159\n\n\nbeta_prime\nbeta_prime\n0.502\n0.111\n0.284\n0.719\n\n\nbeta_ads\nbeta_ads\n-0.732\n0.088\n-0.904\n-0.560\n\n\nbeta_price\nbeta_price\n-0.099\n0.006\n-0.112\n-0.087"
  },
  {
    "objectID": "blog/project4/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "blog/project4/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\n# Set up\nset.seed(123)\nn_samples &lt;- 11000\nburn_in &lt;- 1000\nposterior_draws &lt;- matrix(NA, nrow = n_samples, ncol = 4)\ncolnames(posterior_draws) &lt;- c(\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\")\ncurrent &lt;- c(1.0, 0.5, -0.8, -0.1)\n\n# Proposal SDs: 0.05 for binary var betas, 0.005 for price beta\nproposal_sds &lt;- c(0.05, 0.05, 0.05, 0.005)\n\n# Prior SDs: 5 for binary betas, 1 for price\nprior_sds &lt;- c(5, 5, 5, 1)\n\n# Log-prior\nlog_prior &lt;- function(beta) {\n  sum(dnorm(beta, mean = 0, sd = prior_sds, log = TRUE))\n}\n\n# Log-likelihood (reuse your earlier log-likelihood function)\nlog_post &lt;- function(beta, data) {\n  -loglik_mnl(beta, data) + log_prior(beta)\n}\n\n# Metropolis-Hastings sampler\nposterior_draws[1, ] &lt;- current\nfor (i in 2:n_samples) {\n  proposal &lt;- current + rnorm(4, mean = 0, sd = proposal_sds)\n  log_alpha &lt;- log_post(proposal, conjoint_prepped) - log_post(current, conjoint_prepped)\n  if (log(runif(1)) &lt; log_alpha) {\n    current &lt;- proposal\n  }\n  posterior_draws[i, ] &lt;- current\n}\n\n# Drop burn-in\nposterior_draws &lt;- posterior_draws[(burn_in + 1):n_samples, ]\n\n\npar(mfrow = c(1, 2))\nplot(posterior_draws[, \"beta_netflix\"], type = \"l\", main = \"Trace Plot: beta_netflix\",\n     xlab = \"Iteration\", ylab = \"Value\")\nhist(posterior_draws[, \"beta_netflix\"], breaks = 30, main = \"Posterior: beta_netflix\",\n     xlab = \"Value\", freq = FALSE)\n\n\n\n\n\n\n\n\n\nposterior_summary &lt;- data.frame(\n  Parameter = colnames(posterior_draws),\n  Mean = colMeans(posterior_draws),\n  SD = apply(posterior_draws, 2, sd),\n  Lower_95 = apply(posterior_draws, 2, quantile, probs = 0.025),\n  Upper_95 = apply(posterior_draws, 2, quantile, probs = 0.975)\n)\n\nknitr::kable(posterior_summary, digits = 3, caption = \"Posterior Means, SDs, and 95% Credible Intervals\")\n\n\nPosterior Means, SDs, and 95% Credible Intervals\n\n\n\nParameter\nMean\nSD\nLower_95\nUpper_95\n\n\n\n\nbeta_netflix\nbeta_netflix\n0.929\n0.109\n0.723\n1.147\n\n\nbeta_prime\nbeta_prime\n0.491\n0.112\n0.277\n0.717\n\n\nbeta_ads\nbeta_ads\n-0.735\n0.087\n-0.915\n-0.563\n\n\nbeta_price\nbeta_price\n-0.100\n0.006\n-0.111\n-0.087\n\n\n\n\n\n\ncomparison &lt;- posterior_summary\ncomparison$MLE_Estimate &lt;- c(1.00, 0.50, -0.80, -0.10)  # replace with real MLEs\ncomparison$Difference &lt;- comparison$Mean - comparison$MLE_Estimate\n\nknitr::kable(comparison, digits = 3, caption = \"Bayesian vs MLE Estimates\")\n\n\nBayesian vs MLE Estimates\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMean\nSD\nLower_95\nUpper_95\nMLE_Estimate\nDifference\n\n\n\n\nbeta_netflix\nbeta_netflix\n0.929\n0.109\n0.723\n1.147\n1.0\n-0.071\n\n\nbeta_prime\nbeta_prime\n0.491\n0.112\n0.277\n0.717\n0.5\n-0.009\n\n\nbeta_ads\nbeta_ads\n-0.735\n0.087\n-0.915\n-0.563\n-0.8\n0.065\n\n\nbeta_price\nbeta_price\n-0.100\n0.006\n-0.111\n-0.087\n-0.1\n0.000"
  },
  {
    "objectID": "blog/project4/hw3_questions.html#discussion",
    "href": "blog/project4/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nIf this were real-world data rather than a simulation, the estimated parameters would still provide meaningful insight into average consumer preferences. The fact that \\(\\beta_{\\text{Netflix}} &gt; \\beta_{\\text{Prime}}\\) indicates that, holding price and ad presence constant, consumers on average prefer Netflix over Amazon Prime, and both over Hulu (which is the omitted base category). This ranking aligns with expectations based on brand equity, original content, and perceived value.\nIt also makes intuitive sense that \\(\\beta_{\\text{price}}\\) is negative. This implies that, all else equal, higher prices reduce the utility of a streaming option, decreasing the likelihood of it being selected. The magnitude and sign of this coefficient capture the population’s overall price sensitivity in a straightforward, linear way.\n\n📈 Toward a Multi-Level Model\nThe model we estimated assumes that all consumers share identical preferences. While this works for simulated data or quick insights, real-world choice data often reveal substantial variation across individuals. For example, some consumers may strongly dislike ads, while others tolerate them for a lower price; some may be highly price-sensitive, while others are not.\nTo account for this, we would use a hierarchical (or multi-level) model, in which each respondent \\(i\\) has their own vector of part-worths, \\({\\beta}_i\\). These individual-level coefficients are assumed to be drawn from a population distribution:\n\\[\n{\\beta}_i \\sim \\mathcal{N}({\\mu}, \\Sigma)\n\\]\nThis approach allows us to estimate both the population-level average preferences (\\(\\{\\mu}\\)) and the variation across individuals (\\(\\Sigma\\)).\nTo simulate data from a hierarchical model: - We would first draw \\({\\beta}_i\\) for each respondent from the above distribution. - Then, we would simulate each respondent’s choices using their personalized coefficients.\nTo estimate this kind of model, we would rely on Bayesian methods (e.g., using rstan, brms, or PyMC) or simulated maximum likelihood. Hierarchical models are the standard in real-world conjoint analysis because they better reflect the richness and heterogeneity of consumer behavior."
  },
  {
    "objectID": "blog/project5/hw4_questions.html#dataset",
    "href": "blog/project5/hw4_questions.html#dataset",
    "title": "HW4 Machine Learning",
    "section": "Dataset",
    "text": "Dataset\nWe use the Palmer Penguins dataset, focusing on bill_length_mm and flipper_length_mm.\n\ninstall.packages(c(\"ggplot2\", \"purrr\", \"cluster\", \"class\"))\n\nUpdating HTML index of packages in '.Library'\n\n\nMaking 'packages.html' ... done\n\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(cluster)\nlibrary(class)\n\n\npenguins &lt;- read.csv(\"palmer_penguins.csv\")\n\n# View structure\nstr(penguins)\n\n'data.frame':   333 obs. of  8 variables:\n $ species          : chr  \"Adelie\" \"Adelie\" \"Adelie\" \"Adelie\" ...\n $ island           : chr  \"Torgersen\" \"Torgersen\" \"Torgersen\" \"Torgersen\" ...\n $ bill_length_mm   : num  39.1 39.5 40.3 36.7 39.3 38.9 39.2 41.1 38.6 34.6 ...\n $ bill_depth_mm    : num  18.7 17.4 18 19.3 20.6 17.8 19.6 17.6 21.2 21.1 ...\n $ flipper_length_mm: int  181 186 195 193 190 181 195 182 191 198 ...\n $ body_mass_g      : int  3750 3800 3250 3450 3650 3625 4675 3200 3800 4400 ...\n $ sex              : chr  \"male\" \"female\" \"female\" \"female\" ...\n $ year             : int  2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\npenguins_clean &lt;- na.omit(penguins[, c(\"bill_length_mm\", \"flipper_length_mm\")])\n\nTo explore the behavior of the K-Means clustering algorithm in detail, we implemented a custom version from scratch rather than relying on pre-built functions. This approach helps visualize the iterative nature of K-Means, where cluster centers are repeatedly updated based on distance calculations and group reassignments until convergence. The function below initializes random centroids, assigns data points to the nearest cluster, and updates the cluster means iteratively while tracking the history of centroids for potential animation or diagnostics.\n\nrun_kmeans &lt;- function(df, k, max_iters = 100, random_state = 42) {\n  set.seed(random_state)\n  total_points &lt;- nrow(df)\n  centers &lt;- df[sample(seq_len(total_points), k), , drop = FALSE]\n  snapshots &lt;- list(centers)\n\n  for (step in seq_len(max_iters)) {\n    # Compute pairwise distances\n    combined &lt;- rbind(centers, df)\n    dmat &lt;- as.matrix(dist(combined))[1:k, (k+1):(k+total_points)]\n    \n    # Assign cluster based on nearest center\n    assigned &lt;- apply(dmat, 2, which.min)\n    \n    # Update centers\n    updated &lt;- map_dfr(1:k, function(i) colMeans(df[assigned == i, , drop = FALSE]))\n    snapshots[[step + 1]] &lt;- updated\n    \n    # Stop if convergence\n    if (all(abs(as.matrix(centers) - as.matrix(updated)) &lt; 1e-6)) break\n    centers &lt;- updated\n  }\n\n  return(list(assignments = assigned, centers = centers, trace = snapshots))\n}\n\nBefore visualizing the clusters, we apply our previously defined K-Means function to the cleaned dataset. This involves converting the relevant features into a numeric matrix format, running the algorithm with k = 3, and then appending the resulting cluster labels to the original dataset for plotting and evaluation.\nAfter clustering, we use ggplot2 to create a scatter plot that illustrates how the data points are grouped. Each point is colored by its assigned cluster, and the final centroids are overlaid as red “X” marks. This helps visually assess how well the algorithm separated the data based on the selected features.\n\ninput_matrix &lt;- as.matrix(penguins_clean)\nkmeans_result &lt;- run_kmeans(input_matrix, k = 3)\npenguins_clean$label_custom &lt;- factor(kmeans_result$assignments)\n\nfinal_centers &lt;- as.data.frame(kmeans_result$centers)\ncolnames(final_centers) &lt;- colnames(penguins_clean[, c(\"bill_length_mm\", \"flipper_length_mm\")])\n\n# Plot clusters with centroids\nggplot(penguins_clean, aes(x = bill_length_mm, y = flipper_length_mm, color = label_custom)) +\n  geom_point(size = 3) +\n  geom_point(data = final_centers, \n             aes(x = bill_length_mm, y = flipper_length_mm), \n             color = \"red\", shape = 4, size = 5, stroke = 2) +\n  labs(title = \"Clusters from Custom K-Means Algorithm\",\n       x = \"Bill Length (mm)\", y = \"Flipper Length (mm)\",\n       color = \"Cluster\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTo validate our custom algorithm, we compare its results against R’s built-in kmeans() function. We use the same number of clusters and input features to ensure consistency, and visualize the resulting groupings. The centroids computed by kmeans() are also added to the plot, allowing for a side-by-side visual and structural comparison.\n\n# Run R's native kmeans function\nresult_builtin &lt;- kmeans(input_matrix, centers = 3)\npenguins_clean$label_builtin &lt;- factor(result_builtin$cluster)\n\n# Format the built-in centroids for plotting\nbuiltin_centers &lt;- as.data.frame(result_builtin$centers)\ncolnames(builtin_centers) &lt;- colnames(penguins_clean[, c(\"bill_length_mm\", \"flipper_length_mm\")])\n\n# Visualization of built-in clustering results\nggplot(penguins_clean, aes(x = bill_length_mm, y = flipper_length_mm, color = label_builtin)) +\n  geom_point(size = 3) +\n  geom_point(data = builtin_centers, \n             aes(x = bill_length_mm, y = flipper_length_mm), \n             color = \"red\", shape = 4, size = 5, stroke = 2) +\n  labs(title = \"Clustering via Built-in kmeans()\",\n       x = \"Bill Length (mm)\", y = \"Flipper Length (mm)\",\n       color = \"Cluster\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTo determine a suitable number of clusters for the penguin dataset, we performed quantitative evaluation using two well-known methods: the Within-Cluster Sum of Squares (WCSS) and the Silhouette Score. WCSS measures compactness by quantifying the total variance within each cluster, while the silhouette score captures how well-separated the clusters are relative to one another. By plotting these metrics for a range of cluster values (K = 2 to K = 7), we can assess at which point increasing the number of clusters yields diminishing returns and observe where cluster cohesion and separation are maximized.\n\n# Load necessary libraries\nlibrary(cluster)     # for silhouette()\nlibrary(ggplot2)\n\n# Read and clean the dataset\npenguins_raw &lt;- read.csv(\"palmer_penguins.csv\")\npenguins_trimmed &lt;- na.omit(penguins_raw[, c(\"bill_length_mm\", \"flipper_length_mm\")])\n\n# Initialize empty vectors for WCSS and silhouette scores\nwcss_values &lt;- numeric()\nsilhouette_scores &lt;- numeric()\n\n# Loop through cluster counts from 2 to 7\nfor (k in 2:7) {\n  model_k &lt;- kmeans(penguins_trimmed, centers = k, nstart = 25)\n  wcss_values[k - 1] &lt;- model_k$tot.withinss\n  \n  sil &lt;- silhouette(model_k$cluster, dist(penguins_trimmed))\n  silhouette_scores[k - 1] &lt;- mean(sil[, 3])\n}\n\n# Plot WCSS (elbow method)\nplot(2:7, wcss_values, type = \"b\", pch = 19, col = \"steelblue\",\n     xlab = \"Number of Clusters (K)\", ylab = \"WCSS\",\n     main = \"Elbow Plot: Within-Cluster Sum of Squares\")\n\n\n\n\n\n\n\n# Plot silhouette scores\nplot(2:7, silhouette_scores, type = \"b\", pch = 19, col = \"forestgreen\",\n     xlab = \"Number of Clusters (K)\", ylab = \"Average Silhouette Width\",\n     main = \"Silhouette Plot: Cluster Cohesion and Separation\")\n\n\n\n\n\n\n\n\n\n🧠 Interpretation of the Silhouette Score Plot\nThe plot clearly shows that the highest silhouette score occurs at K = 2, with an average silhouette width exceeding 0.60. This suggests that when the dataset is partitioned into two clusters, the resulting groupings are both tight and well-separated — the ideal condition for clustering quality.\nAs the number of clusters increases beyond 2, the average silhouette score steadily declines, indicating that the new cluster boundaries introduce less coherent groupings or lead to more overlap between clusters. Notably, the score drops sharply at K = 3, and continues to decline until K = 6, with only a minor rebound at K = 7 — but still far below the peak at K = 2.\n\n\n✅ Conclusion\nBased on this silhouette plot alone, the optimal number of clusters for this dataset — when considering only bill length and flipper length — is most likely K = 2, as it offers the best balance between intra-cluster cohesion and inter-cluster separation. This result may differ from the WCSS (elbow method) interpretation, which should be considered in combination for a more robust decision."
  },
  {
    "objectID": "blog/project5/hw4_questions.html#a.-k-nearest-neighbors",
    "href": "blog/project5/hw4_questions.html#a.-k-nearest-neighbors",
    "title": "HW4 Machine Learning",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\n\n📐 Generating the Training Dataset\nTo illustrate the behavior of the K-Nearest Neighbors algorithm, I began by generating a synthetic dataset consisting of two features (x1 and x2), each sampled from a uniform distribution over the interval [-3, 3]. A non-linear decision boundary was defined using the function sin(4x1) + x1, and the binary outcome variable was determined by whether x2 lay above or below this boundary. The resulting dataset simulates a classification problem with a complex boundary.\n\n# Create training data\nset.seed(42)\nn_train &lt;- 100\nfeature1 &lt;- runif(n_train, -3, 3)\nfeature2 &lt;- runif(n_train, -3, 3)\nboundary_train &lt;- sin(4 * feature1) + feature1\nlabels_train &lt;- ifelse(feature2 &gt; boundary_train, 1, 0) |&gt; as.factor()\n\ndata_train &lt;- data.frame(x1 = feature1, x2 = feature2, y = labels_train)\n\n\n\n🖼 Visualizing the Data and Decision Boundary\nTo understand the separation between classes, I plotted the synthetic training data in a scatterplot. The points were colored by class membership, and a dashed curve representing the boundary function was overlaid to show the non-linear dividing line.\n\nggplot(data_train, aes(x = x1, y = x2, color = y)) +\n  geom_point(size = 3) +\n  stat_function(fun = function(x) sin(4 * x) + x, color = \"black\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Synthetic Dataset with Nonlinear Boundary\",\n       x = \"x1\", y = \"x2\", color = \"Class\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n🧾 Creating an Independent Test Set\nTo evaluate generalization, a test dataset was generated using the same process but initialized with a different random seed. This ensured that the test data was independently sampled while following the same structural rule for classification.\n\n# Generate test set\nset.seed(999)\nn_test &lt;- 100\nx1_new &lt;- runif(n_test, -3, 3)\nx2_new &lt;- runif(n_test, -3, 3)\nboundary_test &lt;- sin(4 * x1_new) + x1_new\nlabels_test &lt;- ifelse(x2_new &gt; boundary_test, 1, 0) |&gt; as.factor()\n\ndata_test &lt;- data.frame(x1 = x1_new, x2 = x2_new, y = labels_test)\n\n\n\n🧮 Implementing KNN Manually\nA custom function was developed to implement the KNN algorithm from scratch. For each point in the test set, Euclidean distances to all training points were calculated, the k nearest neighbors identified, and the most common class among them selected as the predicted label.\n\n# Custom implementation of KNN\nknn_from_scratch &lt;- function(x_train, y_train, x_test, k = 5) {\n  predictions &lt;- character(nrow(x_test))\n  \n  for (i in 1:nrow(x_test)) {\n    dist_vals &lt;- sqrt(rowSums((t(t(x_train) - x_test[i, ]))^2))\n    neighbor_idx &lt;- order(dist_vals)[1:k]\n    neighbor_labels &lt;- y_train[neighbor_idx]\n    predictions[i] &lt;- names(sort(table(neighbor_labels), decreasing = TRUE))[1]\n  }\n  \n  return(as.factor(predictions))\n}\n\n# Run KNN manually for k = 5\nmanual_results &lt;- knn_from_scratch(x_train = data_train[, 1:2],\n                                   y_train = data_train$y,\n                                   x_test = data_test[, 1:2],\n                                   k = 5)\n\n\n\n🔁 Validating Against a Built-In Function\nTo verify the correctness of the manual implementation, I compared its predictions with the results of R’s built-in class::knn() function. A confusion matrix was used to confirm that both methods produced closely aligned classifications.\n\nlibrary(class)\n\n# Built-in knn classifier\nbuilt_in_results &lt;- knn(train = data_train[, 1:2],\n                        test = data_test[, 1:2],\n                        cl = data_train$y,\n                        k = 5)\n\n# Compare outputs\ntable(Manual = manual_results, BuiltIn = built_in_results)\n\n      BuiltIn\nManual  0  1\n     0 38 62\n\n\n\n\n📈 Evaluating Performance Across K Values\nTo explore how model performance changes with different neighborhood sizes, I ran the manual KNN classifier for values of k from 1 to 30. For each run, I computed the classification accuracy on the test set and plotted the results to observe the relationship between k and predictive success.\n\n# Track accuracy across different k values\nk_values &lt;- 1:30\naccuracy_scores &lt;- numeric(length(k_values))\n\ntrue_y &lt;- factor(data_test$y, levels = levels(data_train$y))\n\nfor (k in k_values) {\n  preds_k &lt;- knn_from_scratch(x_train = data_train[, 1:2],\n                               y_train = data_train$y,\n                               x_test = data_test[, 1:2],\n                               k = k)\n  \n  preds_k &lt;- factor(preds_k, levels = levels(data_train$y))\n  accuracy_scores[k] &lt;- mean(preds_k == true_y) * 100\n}\n\n# Plot accuracy vs. k\nplot(k_values, accuracy_scores, type = \"b\", pch = 19,\n     xlab = \"Number of Neighbors (k)\",\n     ylab = \"Classification Accuracy (%)\",\n     main = \"KNN Accuracy Across Different k Values\",\n     col = \"blue\")\n\n\n\n\n\n\n\n\n\n\n🧠 Interpretation\nThe plot provides insight into how the KNN algorithm performs as k increases. Lower values of k typically lead to higher variance (more sensitive to noise), while higher values may result in over-smoothing. The optimal value of k is suggested by the peak of the accuracy curve, indicating the best trade-off between bias and variance in this non-linear classification setting."
  }
]